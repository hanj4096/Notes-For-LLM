# 编写注意力机制

&nbsp;
## 主要章节代码

- [01_main-chapter-code](01_main-chapter-code) 包含主要章节代码。

&nbsp;
## 补充材料

- [02_bonus_efficient-multihead-attention](02_bonus_efficient-multihead-attention) 实现并比较了多头注意力的不同实现变体
- [03_understanding-buffers](03_understanding-buffers) 解释了 PyTorch 缓冲区背后的概念，这些缓冲区用于在第三章中实现因果注意力机制



在下面的视频中，我提供了一个代码跟随会话，涵盖了一些章节内容作为补充材料。

<br>
<br>

[![视频链接](https://img.youtube.com/vi/-Ll8DtpNtvk/0.jpg)](https://www.youtube.com/watch?v=-Ll8DtpNtvk)
