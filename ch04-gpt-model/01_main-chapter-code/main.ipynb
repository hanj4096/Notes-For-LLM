{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce9295b2-182b-490b-8325-83a67c4a001d",
   "metadata": {},
   "source": [
    "# 第4章：从零实现GPT模型以生成文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9eac223-a125-40f7-bacc-bd0d890450c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.8\n",
      "torch version: 2.9.1\n",
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"matplotlib version:\", version(\"matplotlib\"))\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7da97ed-e02f-4d7f-b68e-a0eba3716e02",
   "metadata": {},
   "source": [
    "- 在本章中，我们实现一个类似GPT的大语言模型架构；下一章将重点训练这个大语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f11e0-4434-4979-9dee-e1207df0eb01",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/01.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe99ab-0bcf-4778-a6b5-6db81fb826ef",
   "metadata": {},
   "source": [
    "## 4.1 实现LLM架构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72d1ff-d82d-4e33-a88e-3c1a8831797b",
   "metadata": {},
   "source": [
    "- 第一章讨论了像GPT和Llama这样的模型，它们按顺序生成单词，并且基于原始transformer架构的解码器部分\n",
    "- 因此，这些大语言模型通常被称为\"类解码器\"大语言模型\n",
    "- 与传统的深度学习模型相比，大语言模型更大，主要是由于它们拥有大量的参数，而不是代码量\n",
    "- 我们将看到大语言模型架构中有许多元素是重复的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5213e9-bd1c-437e-aee8-f5e8fb717251",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/02.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d43f5e2-fb51-434a-b9be-abeef6b98d99",
   "metadata": {},
   "source": [
    "- 在前面的章节中，为了便于说明，我们使用了较小的嵌入维度用于token输入和输出，确保它们能在一页上显示\n",
    "- 在本章中，我们考虑类似于小型GPT-2模型的嵌入和模型大小\n",
    "- 我们将具体编写最小的GPT-2模型（1.24亿参数）的架构，如Radford等人[Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)中所述（注意，初始报告将其列为1.17亿参数，但后来在模型权重仓库中进行了更正）\n",
    "- 第六章将展示如何将预训练权重加载到我们的实现中，这将与3.45亿、7.62亿和15.42亿参数的模型大小兼容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21baa14d-24b8-4820-8191-a2808f7fbabc",
   "metadata": {},
   "source": [
    "- 1.24亿参数GPT-2模型的配置详情包括："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed66875-1f24-445d-add6-006aae3c5707",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12fcd28-d210-4c57-8be6-06cfcd5d73a4",
   "metadata": {},
   "source": [
    "- 我们使用简短的变量名，以避免后续代码行过长\n",
    "- `\"vocab_size\"` 表示词汇表大小为50,257个单词，由第二章讨论的BPE分词器支持\n",
    "- `\"context_length\"` 表示模型的最大输入token数量，由第二章介绍的位置嵌入支持\n",
    "- `\"emb_dim\"` 是token输入的嵌入大小，将每个输入token转换为768维向量\n",
    "- `\"n_heads\"` 是第三章实现的多头注意力机制中的注意力头数\n",
    "- `\"n_layers\"` 是模型内transformer块的数量，我们将在后续章节中实现\n",
    "- `\"drop_rate\"` 是dropout机制的强度，在第三章中讨论过；0.1表示在训练期间丢弃10%的隐藏单元以减轻过拟合\n",
    "- `\"qkv_bias\"` 决定多头注意力机制（来自第三章）中的`Linear`层在计算查询（Q）、键（K）和值（V）张量时是否应包含偏置向量；我们将禁用此选项，这是现代大语言模型的标准做法；但是，我们稍后在第五章中将OpenAI的预训练GPT-2权重加载到我们的重新实现中时会重新讨论这一点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adce779-857b-4418-9501-12a7f3818d88",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/03.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619c2eed-f8ea-4ff5-92c3-feda0f29b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入PyTorch核心库\n",
    "import torch  # PyTorch张量计算库，提供基础张量操作和自动微分功能\n",
    "import torch.nn as nn  # PyTorch神经网络模块，包含各种层和模型基类\n",
    "\n",
    "\n",
    "# 定义DummyGPTModel类：这是一个占位符GPT模型，用于演示GPT架构的整体结构\n",
    "# 继承自nn.Module，这是PyTorch中所有神经网络模块的基类\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        # 调用父类nn.Module的初始化方法，确保模型正确注册为PyTorch模块\n",
    "        super().__init__()\n",
    "        \n",
    "        # 创建token嵌入层：将词汇表中的每个token（单词）映射到高维向量空间\n",
    "        # cfg[\"vocab_size\"]: 词汇表大小，例如50257（GPT-2的词汇表大小）\n",
    "        # cfg[\"emb_dim\"]: 嵌入维度，例如768（每个token被转换为768维向量）\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        \n",
    "        # 创建位置嵌入层：为序列中的每个位置分配一个可学习的嵌入向量\n",
    "        # cfg[\"context_length\"]: 上下文长度，例如1024（模型能处理的最大token数）\n",
    "        # cfg[\"emb_dim\"]: 嵌入维度，必须与token嵌入维度相同，以便相加\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        \n",
    "        # 创建dropout层：在训练时随机将部分神经元输出置为0，防止过拟合\n",
    "        # cfg[\"drop_rate\"]: dropout率，例如0.1（表示10%的神经元会被随机置零）\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # 创建transformer块序列：这是GPT模型的核心，包含多层transformer块\n",
    "        # 使用占位符DummyTransformerBlock，后续会被真正的TransformerBlock替换\n",
    "        # cfg[\"n_layers\"]: transformer块的数量，例如12（GPT-2 small有12层）\n",
    "        # *[...] 语法用于解包列表，将多个块传递给Sequential\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # 创建最终层归一化：在输出层之前对特征进行归一化\n",
    "        # 使用占位符DummyLayerNorm，后续会被真正的LayerNorm替换\n",
    "        # cfg[\"emb_dim\"]: 嵌入维度，用于指定归一化的特征维度\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        \n",
    "        # 创建输出头：将模型内部表示转换为词汇表上的概率分布\n",
    "        # cfg[\"emb_dim\"]: 输入维度（嵌入维度）\n",
    "        # cfg[\"vocab_size\"]: 输出维度（词汇表大小）\n",
    "        # bias=False: 不使用偏置项，这是GPT-2的设计选择\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        # 前向传播函数：定义数据如何通过模型\n",
    "        \n",
    "        # 获取输入张量的形状信息\n",
    "        # batch_size: 批次大小（同时处理的样本数）\n",
    "        # seq_len: 序列长度（每个样本的token数量）\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        \n",
    "        # 将输入的token索引转换为嵌入向量\n",
    "        # in_idx: 形状为[batch_size, seq_len]的整数张量，包含token的索引\n",
    "        # 输出: 形状为[batch_size, seq_len, emb_dim]的浮点张量\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        \n",
    "        # 生成位置嵌入：为序列中的每个位置创建嵌入向量\n",
    "        # torch.arange(seq_len): 创建[0, 1, 2, ..., seq_len-1]的位置索引\n",
    "        # device=in_idx.device: 确保位置索引在与输入相同的设备上（CPU或GPU）\n",
    "        # 输出: 形状为[seq_len, emb_dim]的位置嵌入，会自动广播到[batch_size, seq_len, emb_dim]\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        \n",
    "        # 将token嵌入和位置嵌入相加：这是transformer架构的标准做法\n",
    "        # 通过相加，模型同时获得token的语义信息和位置信息\n",
    "        # 输出: 形状为[batch_size, seq_len, emb_dim]\n",
    "        x = tok_embeds + pos_embeds\n",
    "        \n",
    "        # 应用dropout：在训练时随机置零部分神经元，防止过拟合\n",
    "        # 在推理时（eval模式），dropout会自动关闭\n",
    "        x = self.drop_emb(x)\n",
    "        \n",
    "        # 通过transformer块序列：这是模型的核心处理部分\n",
    "        # 每个transformer块包含自注意力机制和前馈网络\n",
    "        # 输出: 形状仍为[batch_size, seq_len, emb_dim]，但特征已被多层处理\n",
    "        x = self.trf_blocks(x)\n",
    "        \n",
    "        # 应用最终层归一化：在输出之前对特征进行归一化\n",
    "        # 这有助于稳定训练和提高模型性能\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        # 通过输出头：将内部表示转换为词汇表上的logits（未归一化的分数）\n",
    "        # logits: 形状为[batch_size, seq_len, vocab_size]\n",
    "        # 每个位置都有一个长度为vocab_size的向量，表示该位置预测每个token的分数\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        # 返回logits：这些分数可以通过softmax转换为概率分布\n",
    "        # 概率最高的token就是模型预测的下一个token\n",
    "        return logits\n",
    "\n",
    "\n",
    "# 定义DummyTransformerBlock类：transformer块的占位符\n",
    "# 这个类暂时不做任何处理，只是保持接口一致性，后续会被真正的TransformerBlock替换\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        # 调用父类初始化方法\n",
    "        super().__init__()\n",
    "        # 这是一个简单的占位符，不包含任何实际的层或参数\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播：直接返回输入，不做任何变换\n",
    "        # 这个占位符块用于在实现完整架构之前测试模型的基本结构\n",
    "        return x\n",
    "\n",
    "\n",
    "# 定义DummyLayerNorm类：层归一化的占位符\n",
    "# 这个类暂时不做任何归一化处理，只是保持接口一致性，后续会被真正的LayerNorm替换\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        # 调用父类初始化方法\n",
    "        super().__init__()\n",
    "        # normalized_shape: 需要归一化的维度形状（通常是一个整数或元组）\n",
    "        # eps: 防止除零的小常数（epsilon），在计算方差时添加到分母\n",
    "        # 这些参数只是为了模拟LayerNorm的接口，实际不执行任何操作\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播：直接返回输入，不做任何归一化\n",
    "        # 这个占位符层用于在实现完整架构之前测试模型的基本结构\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9665e8ab-20ca-4100-b9b9-50d9bdee33be",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/04.webp?123\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794b6b6c-d36f-411e-a7db-8ac566a87fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "# 导入tiktoken库：OpenAI开发的高性能BPE（字节对编码）分词器\n",
    "# tiktoken是GPT模型使用的官方分词器，支持GPT-2、GPT-3、GPT-4等模型\n",
    "import tiktoken\n",
    "\n",
    "# 获取GPT-2的分词器：使用与GPT-2训练时相同的编码方式\n",
    "# \"gpt2\"指定使用GPT-2的BPE编码，这确保了与预训练模型的一致性\n",
    "# 返回一个编码器对象，可以将文本转换为token ID，或将token ID转换回文本\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# 初始化批次列表：用于存储多个文本样本的编码结果\n",
    "# 批次处理可以提高模型的计算效率，同时处理多个样本\n",
    "batch = []\n",
    "\n",
    "# 定义第一个文本样本：用于演示的英文句子\n",
    "# 这个句子将被转换为token序列，然后输入到GPT模型\n",
    "txt1 = \"Every effort moves you\"\n",
    "\n",
    "# 定义第二个文本样本：另一个用于演示的英文句子\n",
    "# 注意这个句子比第一个短，后续需要padding或处理不同长度\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "# 将第一个文本编码为token ID并转换为PyTorch张量\n",
    "# tokenizer.encode(txt1): 将文本字符串转换为整数列表（token ID列表）\n",
    "#   例如：\"Every effort moves you\" -> [15496, 11, 314, 716]\n",
    "# torch.tensor(...): 将Python列表转换为PyTorch张量，便于后续计算\n",
    "#   输出形状: [seq_len1]，其中seq_len1是第一个文本的token数量\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "\n",
    "# 将第二个文本编码为token ID并转换为PyTorch张量\n",
    "# tokenizer.encode(txt2): 将文本字符串转换为整数列表\n",
    "#   例如：\"Every day holds a\" -> [15496, 1110, 6622, 257]\n",
    "# torch.tensor(...): 转换为PyTorch张量\n",
    "#   输出形状: [seq_len2]，其中seq_len2是第二个文本的token数量\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "\n",
    "# 将批次列表堆叠成二维张量：将多个一维张量组合成一个批次\n",
    "# torch.stack(batch, dim=0): 沿着第0维（批次维）堆叠张量\n",
    "#   dim=0: 在第一个维度上堆叠，创建新的批次维度\n",
    "#   输入: 两个形状为[seq_len1]和[seq_len2]的一维张量\n",
    "#   输出: 形状为[2, max(seq_len1, seq_len2)]的二维张量\n",
    "#   注意：如果序列长度不同，较短的序列会被padding（填充）到相同长度\n",
    "#   最终形状: [batch_size, seq_len]，例如[2, 4]表示2个样本，每个4个token\n",
    "batch = torch.stack(batch, dim=0)\n",
    "\n",
    "# 打印批次张量：查看编码后的结果\n",
    "# 输出示例: tensor([[6109, 3626, 6100,  345],\n",
    "#                   [6109, 1110, 6622,  257]])\n",
    "# 每一行代表一个文本样本的token ID序列\n",
    "# 这些ID对应词汇表中的特定token，模型将使用这些ID进行前向传播\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "009238cd-0160-4834-979c-309710986bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6755, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fad0fe-895d-4493-9e48-962e2d46c66f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**注意**\n",
    "\n",
    "- 如果您在Windows或Linux上运行此代码，上面的结果值可能如下所示：\n",
    "    \n",
    "```\n",
    "Output shape: torch.Size([2, 4, 50257])\n",
    "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
    "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
    "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
    "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
    "\n",
    "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
    "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
    "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
    "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
    "       grad_fn=<UnsafeViewBackward0>)\n",
    "```\n",
    "\n",
    "- 由于这些只是随机数字，这不是需要担心的问题，您可以继续完成本章的其余部分而不会出现问题\n",
    "- 这种差异的一个可能原因是`nn.Dropout`在不同操作系统上的行为不同，这取决于PyTorch的编译方式，如[PyTorch问题跟踪器](https://github.com/pytorch/pytorch/issues/121595)中讨论的那样\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8332a00-98da-4eb4-b882-922776a89917",
   "metadata": {},
   "source": [
    "## 4.2 用层归一化标准化激活值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cfb81-d59b-4d95-afe3-e43cf095f292",
   "metadata": {},
   "source": [
    "- 层归一化，也称为LayerNorm（[Ba et al. 2016](https://arxiv.org/abs/1607.06450)），将神经网络层的激活值中心化到均值为0，并将它们的方差归一化为1\n",
    "- 这稳定了训练并能够更快地收敛到有效的权重\n",
    "- 层归一化在transformer块内的多头注意力模块之前和之后都应用，我们稍后将实现；它也在最终输出层之前应用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314ac47a-69cc-4597-beeb-65bed3b5910f",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/05.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab49940-6b35-4397-a80e-df8d092770a7",
   "metadata": {},
   "source": [
    "- 让我们通过将一个小输入样本传递到一个简单的神经网络层来了解层归一化的工作原理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1b463-dc3f-44ac-9cdb-9d5b6f64eb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子：确保每次运行代码时生成的随机数相同，便于结果复现和调试\n",
    "# 123是种子值，相同的种子会产生相同的随机数序列\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 创建批次示例：生成一个包含2个训练样本的批次，每个样本有5个特征维度\n",
    "# torch.randn(2, 5): 从标准正态分布（均值为0，标准差为1）中随机采样\n",
    "#   参数说明：\n",
    "#   - 2: 批次大小（batch_size），表示同时处理2个样本\n",
    "#   - 5: 特征维度（feature_dim），每个样本有5个特征\n",
    "#   输出形状: [2, 5]，即2行5列的二维张量\n",
    "#   每一行代表一个样本，每一列代表一个特征\n",
    "batch_example = torch.randn(2, 5) \n",
    "\n",
    "# 定义神经网络层：使用Sequential容器将多个层按顺序组合\n",
    "# nn.Sequential: 按顺序执行多个层的容器，数据会依次通过每个层\n",
    "# nn.Linear(5, 6): 线性（全连接）层\n",
    "#   参数说明：\n",
    "#   - 5: 输入特征维度（in_features），接收5维输入\n",
    "#   - 6: 输出特征维度（out_features），输出6维特征\n",
    "#   功能：执行线性变换 y = xW^T + b，其中W是权重矩阵，b是偏置向量\n",
    "#   权重矩阵形状: [6, 5]，偏置向量形状: [6]\n",
    "# nn.ReLU(): 修正线性单元激活函数\n",
    "#   功能：ReLU(x) = max(0, x)，将负值置为0，正值保持不变\n",
    "#   作用：引入非线性，使网络能够学习复杂的模式\n",
    "# 整体流程：输入[2, 5] -> Linear层 -> [2, 6] -> ReLU激活 -> [2, 6]\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "\n",
    "# 前向传播：将批次数据通过神经网络层进行处理\n",
    "# layer(batch_example): 调用layer的forward方法，执行前向传播\n",
    "#   输入: batch_example，形状为[2, 5]\n",
    "#   处理过程：\n",
    "#     1. 通过Linear层：执行矩阵乘法 [2, 5] × [5, 6]^T = [2, 6]\n",
    "#     2. 通过ReLU层：对每个元素应用ReLU函数，负值变为0\n",
    "#   输出: out，形状为[2, 6]\n",
    "#   含义：每个样本的5个特征被转换为6个新的特征表示\n",
    "out = layer(batch_example)\n",
    "\n",
    "# 打印输出结果：查看经过神经网络层处理后的数据\n",
    "# 输出示例形状: [2, 6]，包含2个样本，每个样本6个特征\n",
    "# 由于ReLU的作用，输出中的负值会被置为0\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fccc29e-71fc-4c16-898c-6137c6ea5d2e",
   "metadata": {},
   "source": [
    "- 让我们计算上面2个输入中每个输入的均值和方差："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9888f79e-8e69-44aa-8a19-cd34292adbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 计算均值：对每个样本的特征维度计算平均值\n",
    "# out.mean(dim=-1, keepdim=True): 沿着最后一个维度计算均值\n",
    "#   参数说明：\n",
    "#   - dim=-1: 指定计算均值的维度\n",
    "#     -1表示最后一个维度，对于形状[2, 6]的张量，-1就是第1维（特征维）\n",
    "#     即沿着特征维度（列方向）计算，对每个样本的6个特征求平均\n",
    "#   - keepdim=True: 保持维度，使输出形状与输入兼容\n",
    "#     如果keepdim=False，输出形状会是[2]（一维）\n",
    "#     如果keepdim=True，输出形状是[2, 1]（二维，保持原始维度数）\n",
    "#   输入: out，形状为[2, 6]（2个样本，每个6个特征）\n",
    "#   计算过程：对每一行的6个特征值求平均\n",
    "#     样本1: mean1 = (f1 + f2 + f3 + f4 + f5 + f6) / 6\n",
    "#     样本2: mean2 = (f1 + f2 + f3 + f4 + f5 + f6) / 6\n",
    "#   输出: mean，形状为[2, 1]\n",
    "#     每一行包含对应样本的特征均值\n",
    "#\n",
    "#   具体示例说明\"沿着最后一个维度计算均值\"：\n",
    "#   假设 out = [[1.0, 2.0, 3.0, 4.0, 5.0, 6.0],    # 样本1的6个特征\n",
    "#               [2.0, 4.0, 6.0, 8.0, 10.0, 12.0]]  # 样本2的6个特征\n",
    "#   形状: [2, 6]，其中：\n",
    "#     - 第0维（dim=0）是批次维，有2个样本\n",
    "#     - 第1维（dim=1，即dim=-1）是特征维，每个样本有6个特征\n",
    "#   \n",
    "#   当执行 out.mean(dim=-1, keepdim=True) 时：\n",
    "#     - dim=-1 表示沿着最后一个维度（特征维，即dim=1）计算\n",
    "#     - 对样本1: mean1 = (1.0 + 2.0 + 3.0 + 4.0 + 5.0 + 6.0) / 6 = 3.5\n",
    "#     - 对样本2: mean2 = (2.0 + 4.0 + 6.0 + 8.0 + 10.0 + 12.0) / 6 = 7.0\n",
    "#     - 输出: [[3.5], [7.0]]，形状为[2, 1]\n",
    "#   \n",
    "#   如果使用 dim=0（沿着批次维计算），结果会不同：\n",
    "#     - 对每个特征位置: mean_feat = (样本1的值 + 样本2的值) / 2\n",
    "#     - 输出: [[1.5, 3.0, 4.5, 6.0, 7.5, 9.0]]，形状为[1, 6]\n",
    "#     - 这计算的是跨样本的每个特征的平均值，而不是每个样本的特征平均值\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "\n",
    "# 计算方差：对每个样本的特征维度计算方差\n",
    "# out.var(dim=-1, keepdim=True): 沿着最后一个维度计算方差\n",
    "#   参数说明：\n",
    "#   - dim=-1: 指定计算方差的维度（与mean相同，沿着特征维度）\n",
    "#   - keepdim=True: 保持维度，输出形状为[2, 1]\n",
    "#   输入: out，形状为[2, 6]\n",
    "#   计算过程：对每一行的6个特征值计算方差\n",
    "#     方差公式: var = Σ(xi - mean)² / n\n",
    "#     样本1: var1 = Σ(fi - mean1)² / 6\n",
    "#     样本2: var2 = Σ(fi - mean2)² / 6\n",
    "#   输出: var，形状为[2, 1]\n",
    "#     每一行包含对应样本的特征方差\n",
    "#   注意：默认使用无偏估计（分母为n-1），可以通过unbiased参数控制\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "\n",
    "# 打印均值结果：查看每个样本的特征均值\n",
    "# 输出形状: [2, 1]，包含2个标量值，分别对应2个样本的均值\n",
    "print(\"Mean:\\n\", mean)\n",
    "\n",
    "# 打印方差结果：查看每个样本的特征方差\n",
    "# 输出形状: [2, 1]，包含2个标量值，分别对应2个样本的方差\n",
    "# 方差反映了特征值的分散程度，方差越大说明特征值分布越分散\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052eda3e-b395-48c4-acd4-eb8083bab958",
   "metadata": {},
   "source": [
    "- 归一化独立应用于两个输入（行）中的每一个；使用dim=-1在最后一个维度（在这种情况下是特征维度）上应用计算，而不是行维度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570db83a-205c-4f6f-b219-1f6195dde1a7",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/06.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ecbc7-eb14-4fa1-b5d0-7e1ff9694f99",
   "metadata": {},
   "source": [
    "- 减去均值并除以方差的平方根（标准差）将输入中心化，使其在列（特征）维度上的均值为0，方差为1："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1d1bb9-3341-4c9a-bc2a-d2489bf89cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[9.9341e-09],\n",
      "        [1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac62b90c-7156-4979-9a79-ce1fb92969c1",
   "metadata": {},
   "source": [
    "- 每个输入都中心化到0，单位方差为1；为了提高可读性，我们可以禁用PyTorch的科学计数法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06c34b-c68a-4b36-afbe-b30eda4eca39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.0000],\n",
      "        [0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944fb958-d4ed-43cc-858d-00052bb6b31a",
   "metadata": {},
   "source": [
    "- 上面，我们归一化了每个输入的特征\n",
    "- 现在，使用相同的想法，我们可以实现一个`LayerNorm`类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3333a305-aa3d-460a-bcce-b80662d464d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义LayerNorm类：实现层归一化（Layer Normalization）操作\n",
    "# 层归一化是transformer架构中的关键组件，用于稳定训练和加速收敛\n",
    "# 继承自nn.Module，使其成为PyTorch的神经网络模块\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        # 调用父类nn.Module的初始化方法，确保模块正确注册\n",
    "        super().__init__()\n",
    "        \n",
    "        # 设置epsilon（ε）值：一个很小的常数，用于防止除零错误\n",
    "        # 当计算方差时，如果方差为0，sqrt(var + eps)可以避免除以0的情况\n",
    "        # 1e-5 = 0.00001，这是一个常用的默认值\n",
    "        self.eps = 1e-5\n",
    "        \n",
    "        # 创建可学习的缩放参数（scale parameter）：用于缩放归一化后的特征\n",
    "        # nn.Parameter: 将张量注册为模型参数，使其在训练过程中可以被优化\n",
    "        # torch.ones(emb_dim): 创建形状为[emb_dim]的全1张量作为初始值\n",
    "        #   初始值为1意味着初始时缩放操作不改变数值（乘以1）\n",
    "        # emb_dim: 嵌入维度，例如768，表示每个特征向量的维度\n",
    "        # 这个参数允许模型学习最适合数据的缩放因子\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        \n",
    "        # 创建可学习的偏移参数（shift parameter）：用于偏移归一化后的特征\n",
    "        # nn.Parameter: 将张量注册为模型参数\n",
    "        # torch.zeros(emb_dim): 创建形状为[emb_dim]的全0张量作为初始值\n",
    "        #   初始值为0意味着初始时偏移操作不改变数值（加上0）\n",
    "        # 这个参数允许模型学习最适合数据的偏移量\n",
    "        # scale和shift的组合使得模型可以学习恢复归一化可能丢失的信息\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播函数：执行层归一化的完整计算过程\n",
    "        \n",
    "        # 步骤1：计算均值\n",
    "        # 沿着最后一个维度（特征维度）计算每个样本的均值\n",
    "        # x.mean(dim=-1, keepdim=True):\n",
    "        #   - dim=-1: 沿着最后一个维度计算（对于形状[batch, seq_len, emb_dim]，就是emb_dim维度）\n",
    "        #   - keepdim=True: 保持维度，输出形状为[batch, seq_len, 1]\n",
    "        #   例如：如果x形状是[2, 4, 768]，mean形状是[2, 4, 1]\n",
    "        #   对每个位置的768维特征向量，计算其768个元素的平均值\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        \n",
    "        # 步骤2：计算方差\n",
    "        # 沿着最后一个维度（特征维度）计算每个样本的方差\n",
    "        # x.var(dim=-1, keepdim=True, unbiased=False):\n",
    "        #   - dim=-1: 沿着最后一个维度计算\n",
    "        #   - keepdim=True: 保持维度，输出形状为[batch, seq_len, 1]\n",
    "        #   - unbiased=False: 使用有偏方差估计（分母为n，而不是n-1）\n",
    "        #     这是GPT-2使用的设置，为了与预训练权重兼容\n",
    "        #   方差公式: var = Σ(xi - mean)² / n\n",
    "        #   例如：如果x形状是[2, 4, 768]，var形状是[2, 4, 1]\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        \n",
    "        # 步骤3：归一化\n",
    "        # 执行标准归一化：将特征中心化（减去均值）并标准化（除以标准差）\n",
    "        # (x - mean): 中心化操作，将每个特征向量减去其均值，使均值为0\n",
    "        # torch.sqrt(var + self.eps): 计算标准差（方差的平方根）\n",
    "        #   - var + self.eps: 添加epsilon防止方差为0时出现除零错误\n",
    "        #   - sqrt(...): 计算标准差，用于标准化\n",
    "        # 归一化公式: norm_x = (x - mean) / sqrt(var + eps)\n",
    "        #   结果：归一化后的特征均值为0，标准差为1\n",
    "        #   例如：如果x形状是[2, 4, 768]，norm_x形状也是[2, 4, 768]\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        # 步骤4：应用可学习的缩放和偏移\n",
    "        # 这是层归一化的关键创新：允许模型学习恢复归一化可能丢失的信息\n",
    "        # self.scale * norm_x: 缩放操作，将归一化后的特征乘以可学习的缩放因子\n",
    "        #   - scale形状: [emb_dim]，例如[768]\n",
    "        #   - norm_x形状: [batch, seq_len, emb_dim]\n",
    "        #   - 广播机制：scale会自动广播到每个位置和样本\n",
    "        # + self.shift: 偏移操作，将缩放后的特征加上可学习的偏移量\n",
    "        #   - shift形状: [emb_dim]，例如[768]\n",
    "        #   - 同样通过广播机制应用到所有位置和样本\n",
    "        # 最终公式: output = scale * norm_x + shift\n",
    "        #   这允许模型学习：\n",
    "        #   - 如果scale=1, shift=0：完全归一化（初始状态）\n",
    "        #   - 如果scale和shift被训练调整：可以恢复归一化前的某些信息\n",
    "        # 输出形状: [batch, seq_len, emb_dim]，与输入x相同\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c3908-7544-4808-b8cb-5d0a55bcca72",
   "metadata": {},
   "source": [
    "**缩放和偏移**\n",
    "\n",
    "- 请注意，除了通过减去均值和除以方差来执行归一化之外，我们还添加了两个可训练参数，一个`scale`参数和一个`shift`参数\n",
    "- 初始的`scale`（乘以1）和`shift`（加上0）值没有任何效果；但是，`scale`和`shift`是可训练参数，如果确定这样做会提高模型在训练任务上的性能，大语言模型会在训练期间自动调整它们\n",
    "- 这允许模型学习最适合其处理数据的适当缩放和偏移\n",
    "- 请注意，我们还在计算方差的平方根之前添加一个较小的值（`eps`）；这是为了避免如果方差为0时出现除零错误\n",
    "\n",
    "**有偏方差**\n",
    "- 在上面的方差计算中，设置`unbiased=False`意味着使用公式$\\frac{\\sum_i (x_i - \\bar{x})^2}{n}$来计算方差，其中n是样本大小（这里是特征或列的数量）；此公式不包括贝塞尔校正（在分母中使用`n-1`），因此提供了方差的有偏估计\n",
    "- 对于大语言模型，其中嵌入维度`n`非常大，使用n和`n-1`之间的差异可以忽略不计\n",
    "- 但是，GPT-2在归一化层中使用有偏方差进行训练，这就是为什么我们也采用此设置以与稍后章节中加载的预训练权重兼容\n",
    "\n",
    "- 现在让我们在实践中尝试`LayerNorm`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1000a-e613-4b43-bd90-e54deed8d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94c12de2-1cab-46e0-a099-e2e470353bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[-0.0000],\n",
      "        [ 0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136cfc4-7c89-492e-b120-758c272bca8c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/07.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11190e7d-8c29-4115-824a-e03702f9dd54",
   "metadata": {},
   "source": [
    "## 4.3 使用GELU激活函数实现前馈神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0585dfb-f21e-40e5-973f-2f63ad5cb169",
   "metadata": {},
   "source": [
    "- 在本节中，我们实现一个小的神经网络子模块，它用作大语言模型中transformer块的一部分\n",
    "- 我们从激活函数开始\n",
    "- 在深度学习中，ReLU（修正线性单元）激活函数由于其简单性和在各种神经网络架构中的有效性而被广泛使用\n",
    "- 在大语言模型中，除了传统的ReLU之外，还使用各种其他类型的激活函数；两个值得注意的例子是GELU（高斯误差线性单元）和SwiGLU（Swish门控线性单元）\n",
    "- GELU和SwiGLU是更复杂的平滑激活函数，分别结合了高斯和sigmoid门控线性单元，为深度学习模型提供更好的性能，这与ReLU的简单分段线性函数不同"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d482ce7-e493-4bfc-a820-3ea99f564ebc",
   "metadata": {},
   "source": [
    "- GELU（高斯误差线性单元，[Hendrycks 和 Gimpel 2016](https://arxiv.org/abs/1606.08415)）可以通过多种方式实现；其精确定义为：GELU(x)=x⋅Φ(x)，其中Φ(x)是标准正态分布的累积分布函数。\n",
    "- 在实际中，通常会采用一种计算更高效的近似形式进行实现，即：$\\text{GELU}(x) \\approx 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}} \\cdot \\left(x + 0.044715 \\cdot x^3\\right)\\right]\\right)\n",
    "$（最初的GPT-2模型也是用此近似公式训练的）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f84694b7-95f3-4323-b6d6-0a73df278e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5487d2-2576-4118-80a7-56c4caac2e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 导入matplotlib绘图库：用于创建数据可视化图表\n",
    "# pyplot是matplotlib的接口，提供了类似MATLAB的绘图功能\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 实例化激活函数对象：创建GELU和ReLU激活函数的实例\n",
    "# GELU(): 创建自定义的GELU激活函数实例（之前定义的类）\n",
    "# nn.ReLU(): 创建PyTorch内置的ReLU激活函数实例\n",
    "# 这两个对象将被用于计算不同输入值对应的激活函数输出\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "# 生成样本数据：创建用于绘制激活函数图像的输入数据\n",
    "# torch.linspace(-3, 3, 100): 在-3到3之间均匀生成100个点\n",
    "#   参数说明：\n",
    "#   - -3: 起始值\n",
    "#   - 3: 结束值\n",
    "#   - 100: 点的数量\n",
    "#   输出: 形状为[100]的一维张量，包含[-3.0, -2.94, -2.88, ..., 2.88, 2.94, 3.0]\n",
    "#   这些点将作为激活函数的输入值，用于观察函数在不同输入下的行为\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "\n",
    "# 计算激活函数的输出：对输入数据应用GELU和ReLU激活函数\n",
    "# gelu(x): 将输入x通过GELU激活函数，计算每个输入值对应的GELU输出\n",
    "#   GELU是平滑的非线性函数，对负值也有非零梯度\n",
    "# relu(x): 将输入x通过ReLU激活函数，计算每个输入值对应的ReLU输出\n",
    "#   ReLU是分段线性函数，负值输出为0，正值直接输出\n",
    "# 输出形状: y_gelu和y_relu都是形状为[100]的一维张量\n",
    "#   每个元素对应x中相应位置的激活函数输出值\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "# 创建图形窗口：设置整个图形的尺寸\n",
    "# plt.figure(figsize=(8, 3)):\n",
    "#   - figsize=(8, 3): 设置图形大小为8英寸宽、3英寸高\n",
    "#   这个尺寸适合并排显示两个子图（GELU和ReLU）\n",
    "plt.figure(figsize=(8, 3))\n",
    "\n",
    "# 循环绘制两个激活函数的图像\n",
    "# enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "#   - zip(...): 将输出数据和标签配对，[(y_gelu, \"GELU\"), (y_relu, \"ReLU\")]\n",
    "#   - enumerate(..., 1): 从1开始计数，i的值将是1和2\n",
    "#   这个循环会执行两次，分别绘制GELU和ReLU的图像\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    # 创建子图：在1行2列的布局中，选择第i个子图位置\n",
    "    # plt.subplot(1, 2, i):\n",
    "    #   - 1, 2: 创建1行2列的子图布局（共2个子图）\n",
    "    #   - i: 当前子图的位置（1表示左图，2表示右图）\n",
    "    #   第一次循环：i=1，创建左侧子图（用于GELU）\n",
    "    #   第二次循环：i=2，创建右侧子图（用于ReLU）\n",
    "    plt.subplot(1, 2, i)\n",
    "    \n",
    "    # 绘制函数曲线：在子图中绘制激活函数的图像\n",
    "    # plt.plot(x, y):\n",
    "    #   - x: x轴数据（输入值），形状[100]\n",
    "    #   - y: y轴数据（激活函数输出），形状[100]\n",
    "    #   绘制100个点连接成的曲线，展示激活函数在[-3, 3]区间内的形状\n",
    "    #   注意：需要将PyTorch张量转换为numpy数组，matplotlib才能绘制\n",
    "    #   （如果x和y是张量，matplotlib会自动处理转换）\n",
    "    plt.plot(x, y)\n",
    "    \n",
    "    # 设置子图标题：为每个子图添加标题，说明这是哪个激活函数\n",
    "    # f\"{label} activation function\": 使用f-string格式化字符串\n",
    "    #   第一次循环：label=\"GELU\"，标题为\"GELU activation function\"\n",
    "    #   第二次循环：label=\"ReLU\"，标题为\"ReLU activation function\"\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    \n",
    "    # 设置x轴标签：为x轴添加标签，说明这是输入值\n",
    "    # \"x\": x轴的标签文本，表示输入值\n",
    "    plt.xlabel(\"x\")\n",
    "    \n",
    "    # 设置y轴标签：为y轴添加标签，说明这是激活函数的输出值\n",
    "    # f\"{label}(x)\": 动态生成y轴标签\n",
    "    #   第一次循环：y轴标签为\"GELU(x)\"\n",
    "    #   第二次循环：y轴标签为\"ReLU(x)\"\n",
    "    #   这清楚地表明y轴表示激活函数对输入x的输出\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    \n",
    "    # 显示网格：在子图中显示网格线，便于读取数值\n",
    "    # grid(True): 启用网格显示\n",
    "    #   网格线帮助观察函数值，更容易看出GELU和ReLU的区别\n",
    "    plt.grid(True)\n",
    "\n",
    "# 调整子图布局：自动调整子图之间的间距，避免重叠\n",
    "# plt.tight_layout(): 自动优化子图的间距和边距\n",
    "#   确保两个子图的标题、标签等元素不会重叠或被截断\n",
    "plt.tight_layout()\n",
    "\n",
    "# 显示图形：在Jupyter Notebook中显示绘制好的图像\n",
    "# plt.show(): 渲染并显示图形\n",
    "#   在Jupyter Notebook中，这会直接在输出区域显示两个并排的激活函数图像\n",
    "#   可以直观地比较GELU和ReLU的形状差异：\n",
    "#   - ReLU: 负值部分为0（水平线），正值部分为直线y=x\n",
    "#   - GELU: 平滑的S型曲线，负值部分也有非零输出\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd01662-14cb-43fd-bffd-2d702813de2d",
   "metadata": {},
   "source": [
    "- 正如我们所看到的，ReLU是一个分段线性函数，如果输入为正，则直接输出输入；否则，它输出零\n",
    "- GELU是一个平滑的非线性函数，它近似ReLU，但对于负值具有非零梯度（除了大约-0.75处）\n",
    "\n",
    "- 接下来，让我们实现小的神经网络模块`FeedForward`，我们稍后将在大语言模型的transformer块中使用它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9275c879-b148-4579-a107-86827ca14d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义FeedForward类：实现前馈神经网络（Feed Forward Network，FFN）\n",
    "# 这是transformer块中的一个关键组件，用于对注意力机制输出的特征进行非线性变换\n",
    "# 前馈网络通常采用\"扩展-压缩\"的结构：先扩展到更大维度，再压缩回原始维度\n",
    "# 继承自nn.Module，使其成为PyTorch的神经网络模块\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        # 调用父类nn.Module的初始化方法，确保模块正确注册\n",
    "        super().__init__()\n",
    "        \n",
    "        # 定义前馈网络的层序列：使用Sequential容器按顺序组合多个层\n",
    "        # nn.Sequential: 按顺序执行多个层的容器，数据会依次通过每个层\n",
    "        self.layers = nn.Sequential(\n",
    "            # 第一层：扩展层（Expansion Layer）\n",
    "            # 将嵌入维度扩展到4倍，这是GPT架构的标准设计\n",
    "            # nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]):\n",
    "            #   - 输入维度: cfg[\"emb_dim\"]，例如768\n",
    "            #   - 输出维度: 4 * cfg[\"emb_dim\"]，例如768 * 4 = 3072\n",
    "            #   功能：执行线性变换 y = xW^T + b\n",
    "            #   权重矩阵形状: [3072, 768]，偏置向量形状: [3072]\n",
    "            #   作用：将特征从768维扩展到3072维，增加模型的表达能力\n",
    "            #   这种扩展设计允许模型学习更复杂的特征表示\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            \n",
    "            # 激活函数层：应用GELU激活函数引入非线性\n",
    "            # GELU(): 高斯误差线性单元激活函数\n",
    "            #   功能：GELU(x) ≈ 0.5 * x * (1 + tanh(√(2/π) * (x + 0.044715 * x³)))\n",
    "            #   特点：平滑的非线性函数，对负值也有非零梯度\n",
    "            #   作用：引入非线性变换，使网络能够学习复杂的模式\n",
    "            #   在GPT-2中，GELU比ReLU表现更好，因为它对负值也有响应\n",
    "            #   输入形状: [batch, seq_len, 3072]\n",
    "            #   输出形状: [batch, seq_len, 3072]（形状不变，只是值被激活函数变换）\n",
    "            GELU(),\n",
    "            \n",
    "            # 第二层：压缩层（Compression Layer）\n",
    "            # 将扩展后的维度压缩回原始的嵌入维度\n",
    "            # nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]):\n",
    "            #   - 输入维度: 4 * cfg[\"emb_dim\"]，例如3072\n",
    "            #   - 输出维度: cfg[\"emb_dim\"]，例如768\n",
    "            #   功能：执行线性变换，将3072维特征压缩回768维\n",
    "            #   权重矩阵形状: [768, 3072]，偏置向量形状: [768]\n",
    "            #   作用：将扩展后的特征表示压缩回原始维度，保持输出形状与输入一致\n",
    "            #   这种\"扩展-压缩\"结构允许模型在中间层学习更丰富的表示，然后压缩回原始维度\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "        # 整体数据流：\n",
    "        #   输入: [batch, seq_len, emb_dim] (例如 [2, 4, 768])\n",
    "        #   -> 扩展层: [batch, seq_len, 4*emb_dim] (例如 [2, 4, 3072])\n",
    "        #   -> GELU激活: [batch, seq_len, 4*emb_dim] (形状不变，值被激活)\n",
    "        #   -> 压缩层: [batch, seq_len, emb_dim] (例如 [2, 4, 768])\n",
    "        #   输出: [batch, seq_len, emb_dim] (与输入形状相同)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播函数：定义数据如何通过前馈网络\n",
    "        \n",
    "        # 将输入x通过层序列：依次通过扩展层、GELU激活、压缩层\n",
    "        # self.layers(x): 调用Sequential容器的forward方法\n",
    "        #   输入: x，形状为[batch, seq_len, emb_dim]\n",
    "        #   处理过程：\n",
    "        #     1. 通过第一层Linear：扩展维度 [batch, seq_len, emb_dim] -> [batch, seq_len, 4*emb_dim]\n",
    "        #     2. 通过GELU：应用激活函数，形状不变\n",
    "        #     3. 通过第二层Linear：压缩维度 [batch, seq_len, 4*emb_dim] -> [batch, seq_len, emb_dim]\n",
    "        #   输出: 形状为[batch, seq_len, emb_dim]，与输入形状相同\n",
    "        #   含义：对每个位置的嵌入向量进行非线性变换，增强特征表示能力\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c4976e2-0261-418e-b042-c5be98c2ccaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "print(GPT_CONFIG_124M[\"emb_dim\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcaacfa-3cfc-4c9e-b668-b71a2753145a",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/09.webp?12\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "928e7f7c-d0b1-499f-8d07-4cadb428a6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "\n",
    "# input shape: [batch_size, num_token, emb_size]\n",
    "x = torch.rand(2, 3, 768) \n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8756c5-6b04-443b-93d0-e555a316c377",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/10.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da2a50-04f4-4388-af23-ad32e405a972",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/11.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffcb905-53c7-4886-87d2-4464c5fecf89",
   "metadata": {},
   "source": [
    "## 4.4 添加快捷连接"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae416c-821e-4bfa-a741-8af4ba5db00e",
   "metadata": {},
   "source": [
    "- 接下来，让我们讨论快捷连接背后的概念，也称为跳跃或残差连接\n",
    "- 最初，快捷连接是在用于计算机视觉的深度网络（残差网络）中提出的，以缓解梯度消失问题\n",
    "- 快捷连接为梯度通过网络创建了一条替代的较短路径\n",
    "- 这是通过将一层的输出添加到后续层的输出来实现的，通常跳过中间的一层或多层\n",
    "- 让我们用一个小的示例网络来说明这个想法：\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/12.webp?123\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cfd241-a32e-4601-8790-784b82f2f23e",
   "metadata": {},
   "source": [
    "- 在代码中，它看起来像这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05473938-799c-49fd-86d4-8ed65f94fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义ExampleDeepNeuralNetwork类：用于演示快捷连接（残差连接）的示例网络\n",
    "# 这个类展示了如何在深度神经网络中实现快捷连接，以及快捷连接如何帮助缓解梯度消失问题\n",
    "# 继承自nn.Module，使其成为PyTorch的神经网络模块\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        # 调用父类nn.Module的初始化方法，确保模块正确注册\n",
    "        super().__init__()\n",
    "        \n",
    "        # 保存快捷连接标志：决定是否在前向传播中使用快捷连接\n",
    "        # use_shortcut: 布尔值，True表示使用快捷连接，False表示不使用\n",
    "        # 这个参数允许我们比较有无快捷连接时的梯度流动情况\n",
    "        self.use_shortcut = use_shortcut\n",
    "        \n",
    "        # 创建层列表：使用ModuleList存储多个神经网络层\n",
    "        # nn.ModuleList: 用于存储多个模块的列表，与普通Python列表的区别是\n",
    "        #   ModuleList中的模块会被正确注册为模型的子模块，参数可以被优化器找到\n",
    "        # layer_sizes: 列表，例如[3, 3, 3, 3, 3, 1]，定义了每层的输入输出维度\n",
    "        #   这个网络有5个隐藏层，每层都包含一个Linear层和一个GELU激活函数\n",
    "        self.layers = nn.ModuleList([\n",
    "            # 第1层：从layer_sizes[0]维映射到layer_sizes[1]维\n",
    "            # nn.Sequential: 将Linear层和GELU激活函数组合在一起\n",
    "            #   - nn.Linear: 线性变换层\n",
    "            #   - GELU(): 激活函数，引入非线性\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            \n",
    "            # 第2层：从layer_sizes[1]维映射到layer_sizes[2]维\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            \n",
    "            # 第3层：从layer_sizes[2]维映射到layer_sizes[3]维\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            \n",
    "            # 第4层：从layer_sizes[3]维映射到layer_sizes[4]维\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            \n",
    "            # 第5层：从layer_sizes[4]维映射到layer_sizes[5]维（输出层）\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播函数：定义数据如何通过网络，可选择性地应用快捷连接\n",
    "        \n",
    "        # 遍历每一层：依次通过网络的每一层\n",
    "        # self.layers: ModuleList包含5个Sequential层\n",
    "        # 每次循环处理一层，数据逐步向前传播\n",
    "        for layer in self.layers:\n",
    "            # 计算当前层的输出：将输入x通过当前层进行处理\n",
    "            # layer(x): 调用当前层的forward方法\n",
    "            #   layer是一个Sequential容器，包含Linear层和GELU激活函数\n",
    "            #   输入: x，形状由layer_sizes决定\n",
    "            #   输出: layer_output，形状为当前层的输出维度\n",
    "            layer_output = layer(x)\n",
    "            \n",
    "            # 检查是否可以应用快捷连接：判断是否使用残差连接\n",
    "            # self.use_shortcut: 检查是否启用了快捷连接功能\n",
    "            # x.shape == layer_output.shape: 检查输入和输出形状是否相同\n",
    "            #   快捷连接要求输入和输出形状相同，才能进行元素级相加\n",
    "            #   如果形状不同，无法直接相加，只能使用普通的前向传播\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                # 应用快捷连接：将输入直接加到层输出上（残差连接）\n",
    "                # x + layer_output: 元素级相加，这是残差连接的核心操作\n",
    "                #   数学表示: output = x + F(x)，其中F(x)是当前层的变换\n",
    "                #   作用：\n",
    "                #     - 创建了从输入到输出的直接路径，梯度可以更容易地反向传播\n",
    "                #     - 缓解梯度消失问题，使深层网络更容易训练\n",
    "                #     - 允许网络学习残差（residual），即F(x) = output - x\n",
    "                #   输出形状: 与x和layer_output相同\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                # 不使用快捷连接：直接使用层的输出作为下一层的输入\n",
    "                # 这是传统的前向传播方式，没有残差连接\n",
    "                # 在深层网络中，这种方式可能导致梯度消失问题\n",
    "                x = layer_output\n",
    "        \n",
    "        # 返回最终输出：经过所有层处理后的结果\n",
    "        # 输出形状: [batch_size, layer_sizes[5]]，例如[1, 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "# 定义print_gradients函数：用于打印模型中所有权重参数的梯度信息\n",
    "# 这个函数用于分析梯度流动，比较有无快捷连接时的梯度大小\n",
    "def print_gradients(model, x):\n",
    "    # 前向传播：将输入数据通过模型，计算输出\n",
    "    # model(x): 调用模型的forward方法\n",
    "    #   x: 输入数据，形状由模型的第一层决定\n",
    "    #   output: 模型的输出，形状由模型的最后一层决定\n",
    "    #   在前向传播过程中，PyTorch会构建计算图，为后续的反向传播做准备\n",
    "    output = model(x)\n",
    "    \n",
    "    # 创建目标值：用于计算损失函数的参考值\n",
    "    # torch.tensor([[0.]]): 创建一个形状为[1, 1]的张量，值为0.0\n",
    "    #   这个目标值用于计算均方误差损失\n",
    "    #   形状[1, 1]表示1个样本，1个输出值\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # 计算损失：衡量模型输出与目标值之间的差异\n",
    "    # nn.MSELoss(): 创建均方误差损失函数对象\n",
    "    #   MSE公式: loss = mean((output - target)²)\n",
    "    #   这个损失函数会计算输出和目标之间的平方差的平均值\n",
    "    loss = nn.MSELoss()\n",
    "    \n",
    "    # 计算损失值：将损失函数应用到输出和目标上\n",
    "    # loss(output, target): 计算均方误差\n",
    "    #   如果output接近target（0），损失值会很小\n",
    "    #   如果output远离target，损失值会很大\n",
    "    #   返回一个标量张量，包含损失值\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # 反向传播：计算损失对模型参数的梯度\n",
    "    # loss.backward(): 执行反向传播算法\n",
    "    #   过程：\n",
    "    #     1. 从损失值开始，沿着计算图反向传播\n",
    "    #     2. 使用链式法则计算每个参数的梯度\n",
    "    #     3. 将梯度存储在参数的.grad属性中\n",
    "    #   注意：只有requires_grad=True的参数才会计算梯度\n",
    "    #   这个操作会修改模型参数的.grad属性，但不会更新参数值本身\n",
    "    loss.backward()\n",
    "\n",
    "    # 遍历模型的所有参数：检查每个参数的梯度信息\n",
    "    # model.named_parameters(): 返回模型的所有参数及其名称的迭代器\n",
    "    #   例如：(\"layers.0.0.weight\", Parameter(...)), (\"layers.0.0.bias\", Parameter(...)), ...\n",
    "    #   每个参数都有一个名称（如\"layers.0.0.weight\"）和一个Parameter对象\n",
    "    for name, param in model.named_parameters():\n",
    "        # 只处理权重参数：跳过偏置参数，只关注权重矩阵的梯度\n",
    "        # 'weight' in name: 检查参数名称中是否包含\"weight\"\n",
    "        #   例如：\"layers.0.0.weight\"包含\"weight\"，会被处理\n",
    "        #   而\"layers.0.0.bias\"不包含\"weight\"，会被跳过\n",
    "        if 'weight' in name:\n",
    "            # 打印权重的平均绝对梯度：显示每个权重层的梯度大小\n",
    "            # param.grad: 参数的梯度张量，形状与参数相同\n",
    "            #   .abs(): 计算梯度的绝对值，因为梯度可能为正或负\n",
    "            #   .mean(): 计算所有梯度值的平均值，得到一个标量\n",
    "            #   .item(): 将标量张量转换为Python数值\n",
    "            # 这个值反映了梯度的大小：\n",
    "            #   - 值大：梯度流动良好，参数更新幅度大\n",
    "            #   - 值小：可能存在梯度消失问题，参数更新幅度小\n",
    "            # 通过比较有无快捷连接时的梯度大小，可以验证快捷连接对梯度流动的改善\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39bf277-b3db-4bb1-84ce-7a20caff1011",
   "metadata": {},
   "source": [
    "- 让我们首先打印**没有**快捷连接的梯度值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f43cc-6923-4018-b980-26023086572c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173584925942123\n",
      "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
      "layers.2.0.weight has gradient mean of 0.0007152040489017963\n",
      "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]  \n",
    "\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837fd5d4-7345-4663-97f5-38f19dfde621",
   "metadata": {},
   "source": [
    "- 接下来，让我们打印**有**快捷连接的梯度值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b7c0c2-f9dd-4dd5-b096-a05c48c5f6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694105327129364\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff783a-46f0-49c5-a7a9-26a525764b6e",
   "metadata": {},
   "source": [
    "- 正如我们根据上面的输出所看到的，快捷连接防止梯度在早期层（朝向`layer.0`）中消失\n",
    "- 接下来，当我们在实现transformer块时，将使用这个快捷连接的概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae578ca-e564-42cf-8635-a2267047cdff",
   "metadata": {},
   "source": [
    "## 4.5 在 Transformer 块中连接注意力层和线性层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3daac6f-6545-4258-8f2d-f45a7394f429",
   "metadata": {},
   "source": [
    "- 在本节中，我们现在将之前的概念组合成一个所谓的transformer块\n",
    "- transformer块将前一章的因果多头注意力模块与线性层、我们在前面部分实现的前馈神经网络相结合\n",
    "- 此外，transformer块还使用dropout和快捷连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e8176-e5e3-4152-b1aa-0bbd7891dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入说明：如果本地没有 `previous_chapters.py` 文件，\n",
    "# 可以从 `llms-from-scratch` PyPI 包中导入\n",
    "# 详细信息请参见: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n",
    "# 例如：\n",
    "# from llms_from_scratch.ch03 import MultiHeadAttention\n",
    "\n",
    "# 从 previous_chapters 模块导入 MultiHeadAttention 类\n",
    "# MultiHeadAttention: 多头注意力机制，这是 transformer 架构的核心组件之一\n",
    "#   在之前的章节中已经实现，这里直接导入使用\n",
    "#   多头注意力允许模型同时关注序列的不同表示子空间\n",
    "from previous_chapters import MultiHeadAttention\n",
    "\n",
    "\n",
    "# 定义 TransformerBlock 类：实现 transformer 架构中的一个完整块\n",
    "# 这是 GPT 模型的核心组件，包含自注意力机制和前馈网络，以及残差连接和层归一化\n",
    "# 继承自 nn.Module，使其成为 PyTorch 的神经网络模块\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        # 调用父类 nn.Module 的初始化方法，确保模块正确注册\n",
    "        super().__init__()\n",
    "        \n",
    "        # 创建多头注意力层：这是 transformer 块的第一部分\n",
    "        # MultiHeadAttention: 实现自注意力机制，允许序列中的每个位置关注所有其他位置\n",
    "        #   参数说明：\n",
    "        #   - d_in=cfg[\"emb_dim\"]: 输入嵌入维度，例如 768\n",
    "        #   - d_out=cfg[\"emb_dim\"]: 输出嵌入维度，与输入相同，保持维度一致\n",
    "        #   - context_length=cfg[\"context_length\"]: 上下文长度，例如 1024，定义最大序列长度\n",
    "        #   - num_heads=cfg[\"n_heads\"]: 注意力头的数量，例如 12，每个头关注不同的表示子空间\n",
    "        #   - dropout=cfg[\"drop_rate\"]: dropout 率，例如 0.1，用于防止过拟合\n",
    "        #   - qkv_bias=cfg[\"qkv_bias\"]: 是否在 Q、K、V 线性变换中使用偏置，布尔值\n",
    "        #   输出形状: [batch_size, num_tokens, emb_dim]\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        \n",
    "        # 创建前馈网络层：这是 transformer 块的第二部分\n",
    "        # FeedForward: 实现前馈神经网络，对注意力输出进行非线性变换\n",
    "        #   cfg: 配置字典，包含 emb_dim 等信息\n",
    "        #   前馈网络采用\"扩展-压缩\"结构：先扩展到 4 倍维度，再压缩回原始维度\n",
    "        #   输出形状: [batch_size, num_tokens, emb_dim]\n",
    "        self.ff = FeedForward(cfg)\n",
    "        \n",
    "        # 创建第一个层归一化：用于注意力块之前的归一化\n",
    "        # LayerNorm: 层归一化，对每个样本的特征维度进行归一化\n",
    "        #   cfg[\"emb_dim\"]: 嵌入维度，例如 768，指定需要归一化的特征维度\n",
    "        #   层归一化有助于稳定训练，加速收敛\n",
    "        #   在 GPT-2 架构中，层归一化放在注意力层之前（Pre-LN 结构）\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        \n",
    "        # 创建第二个层归一化：用于前馈网络块之前的归一化\n",
    "        # LayerNorm: 与前一个层归一化相同，但用于前馈网络部分\n",
    "        #   cfg[\"emb_dim\"]: 嵌入维度，必须与第一个层归一化相同\n",
    "        #   每个 transformer 块有两个层归一化：一个用于注意力，一个用于前馈网络\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        \n",
    "        # 创建残差连接的 dropout 层：用于残差连接后的正则化\n",
    "        # nn.Dropout: dropout 层，在训练时随机将部分神经元输出置为 0\n",
    "        #   cfg[\"drop_rate\"]: dropout 率，例如 0.1，表示 10% 的神经元会被随机置零\n",
    "        #   这个 dropout 应用于残差连接之后，有助于防止过拟合\n",
    "        #   注意：在推理时（eval 模式），dropout 会自动关闭\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播函数：定义数据如何通过 transformer 块\n",
    "        # 输入: x，形状为 [batch_size, num_tokens, emb_dim]\n",
    "        # 输出: 形状与输入相同，但特征已被注意力机制和前馈网络处理\n",
    "        \n",
    "        # ========== 第一部分：注意力块（带残差连接） ==========\n",
    "        \n",
    "        # 保存输入作为快捷连接的参考：用于残差连接\n",
    "        # shortcut = x: 保存原始输入 x，后续会与注意力输出相加\n",
    "        #   这是残差连接的关键：创建从输入到输出的直接路径\n",
    "        #   形状: [batch_size, num_tokens, emb_dim]\n",
    "        shortcut = x\n",
    "        \n",
    "        # 应用第一个层归一化：在注意力层之前进行归一化\n",
    "        # self.norm1(x): 对输入进行层归一化\n",
    "        #   归一化公式: (x - mean) / sqrt(var + eps) * scale + shift\n",
    "        #   作用：\n",
    "        #     - 稳定训练：归一化后的数据分布更稳定\n",
    "        #     - 加速收敛：减少内部协变量偏移\n",
    "        #   输出形状: [batch_size, num_tokens, emb_dim]，与输入相同\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # 通过多头注意力层：计算自注意力\n",
    "        # self.att(x): 执行多头注意力计算\n",
    "        #   过程：\n",
    "        #     1. 将输入转换为 Q（查询）、K（键）、V（值）三个矩阵\n",
    "        #     2. 计算注意力分数：attention = softmax(QK^T / sqrt(d_k))V\n",
    "        #     3. 多头机制：将输入分成多个头，分别计算注意力，然后拼接\n",
    "        #   输出形状: [batch_size, num_tokens, emb_dim]\n",
    "        #   每个位置的输出是其他所有位置的加权和，权重由注意力分数决定\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        \n",
    "        # 应用 dropout：在残差连接之前进行正则化\n",
    "        # self.drop_shortcut(x): 随机将部分神经元输出置为 0\n",
    "        #   在训练时，这有助于防止过拟合\n",
    "        #   在推理时（eval 模式），dropout 会自动关闭，所有神经元都参与计算\n",
    "        #   输出形状: [batch_size, num_tokens, emb_dim]\n",
    "        x = self.drop_shortcut(x)\n",
    "        \n",
    "        # 应用残差连接：将原始输入加到注意力输出上\n",
    "        # x + shortcut: 元素级相加，这是残差连接的核心操作\n",
    "        #   数学表示: output = x + shortcut，其中 x 是注意力输出，shortcut 是原始输入\n",
    "        #   作用：\n",
    "        #     - 创建直接路径：梯度可以直接从输出流回输入，缓解梯度消失\n",
    "        #     - 允许学习残差：网络学习的是残差（residual），即 F(x) = output - x\n",
    "        #     - 稳定训练：即使注意力层学习不好，至少可以传递原始信息\n",
    "        #   输出形状: [batch_size, num_tokens, emb_dim]\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # ========== 第二部分：前馈网络块（带残差连接） ==========\n",
    "        \n",
    "        # 保存当前状态作为快捷连接的参考：用于前馈网络的残差连接\n",
    "        # shortcut = x: 保存注意力块的输出，作为前馈网络残差连接的参考\n",
    "        #   这是第二个残差连接：从前馈网络的输入到输出\n",
    "        #   形状: [batch_size, num_tokens, emb_dim]\n",
    "        shortcut = x\n",
    "        \n",
    "        # 应用第二个层归一化：在前馈网络之前进行归一化\n",
    "        # self.norm2(x): 对注意力块的输出进行层归一化\n",
    "        #   与第一个层归一化相同，但应用于不同的数据流\n",
    "        #   输出形状: [batch_size, num_tokens, emb_dim]\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        # 通过前馈网络层：执行非线性变换\n",
    "        # self.ff(x): 执行前馈网络计算\n",
    "        #   过程：\n",
    "        #     1. 扩展层：将嵌入维度扩展到 4 倍（例如 768 → 3072）\n",
    "        #     2. GELU 激活：应用 GELU 激活函数引入非线性\n",
    "        #     3. 压缩层：将维度压缩回原始大小（例如 3072 → 768）\n",
    "        #   输出形状: [batch_size, num_tokens, emb_dim]\n",
    "        #   前馈网络对每个位置独立处理，不涉及位置之间的交互\n",
    "        x = self.ff(x)\n",
    "        \n",
    "        # 应用 dropout：在残差连接之前进行正则化\n",
    "        # self.drop_shortcut(x): 与前一个 dropout 相同，用于防止过拟合\n",
    "        #   输出形状: [batch_size, num_tokens, emb_dim]\n",
    "        x = self.drop_shortcut(x)\n",
    "        \n",
    "        # 应用残差连接：将前馈网络的输入加到输出上\n",
    "        # x + shortcut: 元素级相加，第二个残差连接\n",
    "        #   数学表示: output = x + shortcut，其中 x 是前馈网络输出，shortcut 是前馈网络输入\n",
    "        #   作用：与前一个残差连接相同，创建直接路径，缓解梯度消失\n",
    "        #   输出形状: [batch_size, num_tokens, emb_dim]\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # 返回最终输出：经过注意力机制和前馈网络处理后的结果\n",
    "        # 输出形状: [batch_size, num_tokens, emb_dim]，与输入相同\n",
    "        # 每个位置的嵌入向量都包含了：\n",
    "        #   - 来自注意力机制的其他位置信息\n",
    "        #   - 来自前馈网络的非线性变换\n",
    "        #   - 来自残差连接的原始信息\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b64d16-94a6-4d13-8c85-9494c50478a9",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/13.webp?1\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d2d375-87bd-4153-9040-63a1e6a2b7cb",
   "metadata": {},
   "source": [
    "- 假设我们有2个输入样本，每个样本有4个token，其中每个token是一个768维嵌入向量；然后这个transformer块应用自注意力，接着是线性层，以产生相似大小的输出\n",
    "- 您可以将输出视为我们在前一章讨论的上下文向量的增强版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb45a63-b1f3-4b08-b525-dafbc8228405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "x = torch.rand(2, 4, 768)  # Shape: [batch_size, num_tokens, emb_dim]\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9e4ee4-cf23-4583-b1fd-317abb4fcd13",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/14.webp?1\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46618527-15ac-4c32-ad85-6cfea83e006e",
   "metadata": {},
   "source": [
    "## 4.6 编写GPT模型代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec7d03d-9ff3-4ca3-ad67-01b67c2f5457",
   "metadata": {},
   "source": [
    "- 我们快完成了：现在让我们将transformer块插入到我们在本章开始时编码的架构中，以便获得可用的GPT架构\n",
    "- 请注意，transformer块被重复多次；对于最小的1.24亿参数GPT-2模型，我们重复12次："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b362d-f8c5-48d2-8ebd-722480ac5073",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/15.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e4b5d-ed89-4fdf-9a52-67deee0593bc",
   "metadata": {},
   "source": [
    "- 相应的代码实现，其中`cfg[\"n_layers\"] = 12`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61de39c-d03c-4a32-8b57-f49ac3834857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 GPTModel 类：实现完整的 GPT（Generative Pre-trained Transformer）模型\n",
    "# 这是 GPT 架构的完整实现，包含嵌入层、transformer 块序列和输出层\n",
    "# 继承自 nn.Module，使其成为 PyTorch 的神经网络模块\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        # 调用父类 nn.Module 的初始化方法，确保模块正确注册\n",
    "        super().__init__()\n",
    "        \n",
    "        # 创建 token 嵌入层：将词汇表中的每个 token 映射到高维向量空间\n",
    "        # nn.Embedding: 嵌入层，将离散的 token ID 转换为连续的向量表示\n",
    "        #   cfg[\"vocab_size\"]: 词汇表大小，例如 50257（GPT-2 的词汇表大小）\n",
    "        #   cfg[\"emb_dim\"]: 嵌入维度，例如 768（每个 token 被转换为 768 维向量）\n",
    "        #   权重矩阵形状: [vocab_size, emb_dim]，例如 [50257, 768]\n",
    "        #   功能：将整数索引（token ID）转换为密集的向量表示\n",
    "        #   例如：token ID 1234 -> [0.23, -0.45, 0.67, ..., 0.12]（768 维向量）\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        \n",
    "        # 创建位置嵌入层：为序列中的每个位置分配一个可学习的嵌入向量\n",
    "        # nn.Embedding: 位置嵌入层，将位置索引转换为向量表示\n",
    "        #   cfg[\"context_length\"]: 上下文长度，例如 1024（模型能处理的最大 token 数）\n",
    "        #   cfg[\"emb_dim\"]: 嵌入维度，必须与 token 嵌入维度相同，以便相加\n",
    "        #   权重矩阵形状: [context_length, emb_dim]，例如 [1024, 768]\n",
    "        #   功能：为序列中的每个位置（0, 1, 2, ..., context_length-1）创建位置向量\n",
    "        #   位置嵌入允许模型理解 token 在序列中的位置信息\n",
    "        #   例如：位置 0 -> [0.1, -0.2, 0.3, ..., 0.4]（768 维向量）\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        \n",
    "        # 创建嵌入层的 dropout：在嵌入层之后进行正则化\n",
    "        # nn.Dropout: dropout 层，在训练时随机将部分神经元输出置为 0\n",
    "        #   cfg[\"drop_rate\"]: dropout 率，例如 0.1（表示 10% 的神经元会被随机置零）\n",
    "        #   这个 dropout 应用于 token 嵌入和位置嵌入相加之后\n",
    "        #   作用：防止过拟合，提高模型的泛化能力\n",
    "        #   注意：在推理时（eval 模式），dropout 会自动关闭\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # 创建 transformer 块序列：这是 GPT 模型的核心，包含多层 transformer 块\n",
    "        # nn.Sequential: 按顺序执行多个模块的容器，数据会依次通过每个块\n",
    "        #   *[...] 语法：解包列表，将多个 TransformerBlock 传递给 Sequential\n",
    "        #   TransformerBlock(cfg): 创建 transformer 块，包含注意力机制和前馈网络\n",
    "        #   range(cfg[\"n_layers\"]): 创建 n_layers 个 transformer 块\n",
    "        #     cfg[\"n_layers\"]: transformer 块的数量，例如 12（GPT-2 small 有 12 层）\n",
    "        #   整体结构：输入 -> Block1 -> Block2 -> ... -> BlockN -> 输出\n",
    "        #   每个 transformer 块都会对输入进行注意力计算和非线性变换\n",
    "        #   输出形状: [batch_size, num_tokens, emb_dim]，与输入形状相同\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # 创建最终层归一化：在输出层之前对特征进行归一化\n",
    "        # LayerNorm: 层归一化，对每个样本的特征维度进行归一化\n",
    "        #   cfg[\"emb_dim\"]: 嵌入维度，例如 768，指定需要归一化的特征维度\n",
    "        #   层归一化有助于稳定训练，加速收敛\n",
    "        #   在 GPT-2 架构中，最终层归一化放在输出头之前\n",
    "        #   输出形状: [batch_size, num_tokens, emb_dim]，与输入形状相同\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        \n",
    "        # 创建输出头：将模型内部表示转换为词汇表上的 logits（未归一化的分数）\n",
    "        # nn.Linear: 线性（全连接）层，执行线性变换\n",
    "        #   cfg[\"emb_dim\"]: 输入维度（嵌入维度），例如 768\n",
    "        #   cfg[\"vocab_size\"]: 输出维度（词汇表大小），例如 50257\n",
    "        #   bias=False: 不使用偏置项，这是 GPT-2 的设计选择\n",
    "        #   权重矩阵形状: [vocab_size, emb_dim]，例如 [50257, 768]\n",
    "        #   功能：将每个位置的嵌入向量转换为词汇表上的分数\n",
    "        #   输出 logits 形状: [batch_size, num_tokens, vocab_size]\n",
    "        #   每个位置的 logits 表示该位置预测每个 token 的分数\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        # 前向传播函数：定义数据如何通过 GPT 模型\n",
    "        # 输入: in_idx，形状为 [batch_size, seq_len] 的整数张量，包含 token 的索引\n",
    "        # 输出: logits，形状为 [batch_size, seq_len, vocab_size] 的浮点张量\n",
    "        \n",
    "        # 获取输入张量的形状信息：提取批次大小和序列长度\n",
    "        # in_idx.shape: 获取输入张量的形状，例如 (2, 1024)\n",
    "        #   batch_size: 批次大小（同时处理的样本数），例如 2\n",
    "        #   seq_len: 序列长度（每个样本的 token 数量），例如 1024\n",
    "        #   这些信息用于后续的位置嵌入生成\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        \n",
    "        # 将输入的 token 索引转换为嵌入向量\n",
    "        # self.tok_emb(in_idx): 通过 token 嵌入层将 token ID 转换为向量\n",
    "        #   in_idx: 形状为 [batch_size, seq_len] 的整数张量\n",
    "        #     例如：[[1234, 5678, 9012, ...], [3456, 7890, 1234, ...]]\n",
    "        #   输出: tok_embeds，形状为 [batch_size, seq_len, emb_dim] 的浮点张量\n",
    "        #     例如：[[[0.23, -0.45, ...], [0.67, 0.12, ...], ...], ...]\n",
    "        #   每个 token ID 被转换为一个 emb_dim 维的向量\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        \n",
    "        # 生成位置嵌入：为序列中的每个位置创建嵌入向量\n",
    "        # self.pos_emb(torch.arange(seq_len, device=in_idx.device)):\n",
    "        #   torch.arange(seq_len): 创建位置索引序列 [0, 1, 2, ..., seq_len-1]\n",
    "        #     例如：如果 seq_len=1024，则创建 [0, 1, 2, ..., 1023]\n",
    "        #   device=in_idx.device: 确保位置索引在与输入相同的设备上（CPU 或 GPU）\n",
    "        #     这很重要，因为嵌入层必须在与输入相同的设备上\n",
    "        #   输出: pos_embeds，形状为 [seq_len, emb_dim] 的位置嵌入\n",
    "        #     例如：[[0.1, -0.2, ...], [0.3, 0.4, ...], ..., [0.5, -0.1, ...]]\n",
    "        #   这个位置嵌入会自动广播到 [batch_size, seq_len, emb_dim]\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        \n",
    "        # 将 token 嵌入和位置嵌入相加：这是 transformer 架构的标准做法\n",
    "        # tok_embeds + pos_embeds: 元素级相加，将位置信息注入到 token 嵌入中\n",
    "        #   通过相加，模型同时获得：\n",
    "        #     - token 的语义信息（来自 token 嵌入）\n",
    "        #     - token 的位置信息（来自位置嵌入）\n",
    "        #   输出: x，形状为 [batch_size, seq_len, emb_dim]\n",
    "        #   例如：如果 tok_embeds[0,0] = [0.23, -0.45, ...]，pos_embeds[0] = [0.1, -0.2, ...]\n",
    "        #   则 x[0,0] = [0.33, -0.65, ...]（对应元素相加）\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        \n",
    "        # 应用嵌入层的 dropout：在进入 transformer 块之前进行正则化\n",
    "        # self.drop_emb(x): 随机将部分嵌入向量的元素置为 0\n",
    "        #   在训练时，这有助于防止过拟合\n",
    "        #   在推理时（eval 模式），dropout 会自动关闭，所有元素都参与计算\n",
    "        #   输出形状: [batch_size, seq_len, emb_dim]，与输入相同\n",
    "        x = self.drop_emb(x)\n",
    "        \n",
    "        # 通过 transformer 块序列：这是模型的核心处理部分\n",
    "        # self.trf_blocks(x): 依次通过所有 transformer 块\n",
    "        #   每个 transformer 块包含：\n",
    "        #     - 自注意力机制：允许每个位置关注所有其他位置\n",
    "        #     - 前馈网络：对注意力输出进行非线性变换\n",
    "        #     - 残差连接：缓解梯度消失问题\n",
    "        #     - 层归一化：稳定训练\n",
    "        #   数据流：x -> Block1 -> Block2 -> ... -> BlockN -> x\n",
    "        #   输出形状: [batch_size, seq_len, emb_dim]，与输入形状相同\n",
    "        #   但特征已被多层 transformer 块处理，包含了丰富的上下文信息\n",
    "        x = self.trf_blocks(x)\n",
    "        \n",
    "        # 应用最终层归一化：在输出之前对特征进行归一化\n",
    "        # self.final_norm(x): 对 transformer 块的输出进行层归一化\n",
    "        #   归一化公式: (x - mean) / sqrt(var + eps) * scale + shift\n",
    "        #   作用：\n",
    "        #     - 稳定训练：归一化后的数据分布更稳定\n",
    "        #     - 加速收敛：减少内部协变量偏移\n",
    "        #   输出形状: [batch_size, seq_len, emb_dim]，与输入相同\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        # 通过输出头：将内部表示转换为词汇表上的 logits\n",
    "        # self.out_head(x): 将每个位置的嵌入向量转换为词汇表上的分数\n",
    "        #   输入: x，形状为 [batch_size, seq_len, emb_dim]\n",
    "        #   处理：对每个位置的 emb_dim 维向量执行线性变换\n",
    "        #   输出: logits，形状为 [batch_size, seq_len, vocab_size]\n",
    "        #     例如：如果 batch_size=2, seq_len=1024, vocab_size=50257\n",
    "        #     则 logits 形状为 [2, 1024, 50257]\n",
    "        #   每个位置的 logits 是一个长度为 vocab_size 的向量\n",
    "        #   表示该位置预测词汇表中每个 token 的分数（未归一化）\n",
    "        #   这些分数可以通过 softmax 转换为概率分布\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        # 返回 logits：这些分数用于预测下一个 token\n",
    "        # logits: 形状为 [batch_size, seq_len, vocab_size] 的浮点张量\n",
    "        #   每个位置的 logits 表示该位置预测每个 token 的分数\n",
    "        #   概率最高的 token 就是模型预测的下一个 token\n",
    "        #   在训练时，这些 logits 会与真实标签计算交叉熵损失\n",
    "        #   在推理时，这些 logits 会通过 softmax 转换为概率，然后采样生成文本\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750270f-c45d-4410-8767-a6adbd05d5c3",
   "metadata": {},
   "source": [
    "- 使用1.24亿参数模型的配置，我们现在可以用随机初始权重实例化这个GPT模型，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef94fd9c-4e9d-470d-8f8e-dd23d1bb1f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d616e7a-568b-4921-af29-bd3f4683cd2e",
   "metadata": {},
   "source": [
    "- 我们将在下一章训练这个模型\n",
    "- 但是，关于其大小的快速说明：我们之前将其称为1.24亿参数模型；我们可以按以下方式再次检查这个数字："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb8be4-9d3b-402b-b3da-86b663aac33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d13dd-dd01-4ba6-a2ad-31ca8a9fd660",
   "metadata": {},
   "source": [
    "- 正如我们在上面看到的，这个模型有1.63亿，而不是1.24亿参数；为什么？\n",
    "- 在原始GPT-2论文中，研究人员应用了权重绑定，这意味着他们重用token嵌入层（`tok_emb`）作为输出层，这意味着设置`self.out_head.weight = self.tok_emb.weight`\n",
    "- token嵌入层将50,257维的独热编码输入token投影到768维嵌入表示\n",
    "- 输出层将768维嵌入投影回50,257维表示，以便我们可以将这些转换回单词（下一节将详细介绍）\n",
    "- 因此，嵌入层和输出层具有相同数量的权重参数，正如我们可以根据它们权重矩阵的形状所看到的\n",
    "- 但是，关于其大小的快速说明：我们之前将其称为1.24亿参数模型；我们可以按以下方式再次检查这个数字："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b43233-e9b8-4f5a-b72b-a263ec686982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02259f6-6f79-4c89-a866-4ebeae1c3289",
   "metadata": {},
   "source": [
    "- 在原始GPT-2论文中，研究人员重用了token嵌入矩阵作为输出矩阵\n",
    "- 相应地，如果我们减去输出层的参数数量，我们将得到一个1.24亿参数模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a22e02-50d3-48b3-a4e0-d9863343c164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b03f80-b94c-46e7-9d42-d0df399ff3db",
   "metadata": {},
   "source": [
    "- 在实践中，我发现训练没有权重绑定的模型更容易，这就是为什么我们没有在这里实现它\n",
    "- 但是，我们稍后在第五章加载预训练权重时会重新讨论并应用这个权重绑定的想法\n",
    "- 最后，我们可以按以下方式计算模型的内存需求，这可能是一个有用的参考点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5131a752-fab8-4d70-a600-e29870b33528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "total_size_bytes = total_params * 4\n",
    "\n",
    "# Convert to megabytes\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a3be4-c20a-4657-b4e0-77c97510b47c",
   "metadata": {},
   "source": [
    "- 练习：您可以尝试以下其他配置，这些配置也在[GPT-2论文](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=dOad5HoAAAAJ&citation_for_view=dOad5HoAAAAJ:YsMSGLbcyi4C)中引用。\n",
    "\n",
    "    - **GPT2-small**（我们已经实现的1.24亿配置）：\n",
    "        - \"emb_dim\" = 768\n",
    "        - \"n_layers\" = 12\n",
    "        - \"n_heads\" = 12\n",
    "\n",
    "    - **GPT2-medium:**\n",
    "        - \"emb_dim\" = 1024\n",
    "        - \"n_layers\" = 24\n",
    "        - \"n_heads\" = 16\n",
    "    \n",
    "    - **GPT2-large:**\n",
    "        - \"emb_dim\" = 1280\n",
    "        - \"n_layers\" = 36\n",
    "        - \"n_heads\" = 20\n",
    "    \n",
    "    - **GPT2-XL:**\n",
    "        - \"emb_dim\" = 1600\n",
    "        - \"n_layers\" = 48\n",
    "        - \"n_heads\" = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5d9bc0-95ab-45d4-9378-417628d86e35",
   "metadata": {},
   "source": [
    "## 4.7 文本生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da5deb-6ee0-4b9b-8dd2-abed7ed65172",
   "metadata": {},
   "source": [
    "- 像我们上面实现的GPT模型这样的大语言模型用于一次生成一个单词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caade12a-fe97-480f-939c-87d24044edff",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/16.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7061524-a3bd-4803-ade6-2e3b7b79ac13",
   "metadata": {},
   "source": [
    "- 以下`generate_text_simple`函数实现贪婪解码，这是一种简单快速的文本生成方法\n",
    "- 在贪婪解码中，在每一步，模型选择概率最高的单词（或token）作为其下一个输出（最高的logit对应于最高的概率，因此从技术上讲，我们甚至不需要显式计算softmax函数）\n",
    "- 在下一章中，我们将实现一个更高级的`generate_text`函数\n",
    "- 下图描述了GPT模型在给定输入上下文的情况下如何生成下一个单词token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0f32c-c18c-445e-b294-a879de2aa187",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/17.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b428a9-8764-4b36-80cd-7d4e00595ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 generate_text_simple 函数：使用贪婪解码（greedy decoding）方法生成文本\n",
    "# 这是最简单的文本生成方法，每次选择概率最高的 token\n",
    "# 贪婪解码的优点是速度快、确定性好，但可能生成重复或不够多样化的文本\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # 函数参数说明：\n",
    "    #   model: GPT 模型对象，用于生成预测\n",
    "    #   idx: 输入序列的 token 索引，形状为 (batch, n_tokens) 的整数张量\n",
    "    #        例如：[[1234, 5678, 9012, ...]]，表示一个批次中的 token ID 序列\n",
    "    #   max_new_tokens: 要生成的新 token 数量，例如 50，表示生成 50 个新 token\n",
    "    #   context_size: 模型支持的最大上下文长度，例如 1024\n",
    "    #                 如果输入序列超过这个长度，会被裁剪到最后的 context_size 个 token\n",
    "    \n",
    "    # idx 是 (batch, n_tokens) 形状的数组，包含当前上下文的 token 索引\n",
    "    # 例如：如果 batch=1, n_tokens=10，则 idx 形状为 [1, 10]\n",
    "    \n",
    "    # 循环生成新 token：每次迭代生成一个 token，共生成 max_new_tokens 个\n",
    "    # range(max_new_tokens): 执行 max_new_tokens 次迭代\n",
    "    #   例如：如果 max_new_tokens=50，则循环 50 次，每次生成一个 token\n",
    "    #   每次迭代都会：\n",
    "    #     1. 裁剪上下文到模型支持的长度\n",
    "    #     2. 通过模型获取预测\n",
    "    #     3. 选择概率最高的 token\n",
    "    #     4. 将新 token 添加到序列中\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # 裁剪当前上下文：如果上下文超过模型支持的最大长度，只使用最后的部分\n",
    "        # idx[:, -context_size:]: 使用切片操作提取最后 context_size 个 token\n",
    "        #   参数说明：\n",
    "        #     - : 表示选择所有批次（保持批次维度不变）\n",
    "        #     - -context_size: 从倒数第 context_size 个位置开始\n",
    "        #     - : 表示到序列末尾\n",
    "        #   例如：\n",
    "        #     - 如果 idx 形状为 [1, 20]，context_size=5\n",
    "        #     - 则 idx_cond 形状为 [1, 5]，包含最后 5 个 token\n",
    "        #   原因：GPT 模型有固定的最大上下文长度（如 1024），超过这个长度无法处理\n",
    "        #   通过只使用最后的 token，我们确保模型能够处理输入\n",
    "        #   输出: idx_cond，形状为 (batch, min(n_tokens, context_size))\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # 获取模型预测：通过模型计算每个位置的下一个 token 的 logits\n",
    "        # with torch.no_grad(): 禁用梯度计算，节省内存和计算资源\n",
    "        #   在推理时（文本生成）不需要计算梯度，因为不进行反向传播\n",
    "        #   这可以显著减少内存使用和加速计算\n",
    "        with torch.no_grad():\n",
    "            # model(idx_cond): 将裁剪后的上下文输入模型\n",
    "            #   输入: idx_cond，形状为 (batch, n_tokens_cond)，例如 [1, 5]\n",
    "            #   处理：模型执行前向传播，计算每个位置的 logits\n",
    "            #   输出: logits，形状为 (batch, n_tokens_cond, vocab_size)\n",
    "            #     例如：如果 batch=1, n_tokens_cond=5, vocab_size=50257\n",
    "            #     则 logits 形状为 [1, 5, 50257]\n",
    "            #   每个位置的 logits 表示该位置预测词汇表中每个 token 的分数\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # 只关注最后一个时间步：提取最后一个位置的 logits\n",
    "        # logits[:, -1, :]: 使用切片操作提取最后一个位置的预测\n",
    "        #   参数说明：\n",
    "        #     - : 表示选择所有批次\n",
    "        #     - -1: 选择最后一个位置（最后一个 token）\n",
    "        #     - : 表示选择所有词汇表条目\n",
    "        #   例如：\n",
    "        #     - 输入 logits 形状: (batch, n_tokens_cond, vocab_size) = [1, 5, 50257]\n",
    "        #     - 输出 logits 形状: (batch, vocab_size) = [1, 50257]\n",
    "        #   原因：我们只需要预测下一个 token，所以只需要最后一个位置的 logits\n",
    "        #   最后一个位置的 logits 包含了基于整个上下文的下一个 token 预测\n",
    "        logits = logits[:, -1, :]  # (batch, n_tokens, vocab_size) 变为 (batch, vocab_size)\n",
    "\n",
    "        # 应用 softmax 获取概率分布：将 logits 转换为概率\n",
    "        # torch.softmax(logits, dim=-1): 对最后一个维度应用 softmax 函数\n",
    "        #   logits: 形状为 (batch, vocab_size) 的浮点张量，包含未归一化的分数\n",
    "        #   dim=-1: 指定在最后一个维度（词汇表维度）上应用 softmax\n",
    "        #   softmax 公式: probas[i] = exp(logits[i]) / sum(exp(logits[j])) for all j\n",
    "        #   输出: probas，形状为 (batch, vocab_size) 的浮点张量\n",
    "        #     例如：如果 batch=1, vocab_size=50257，则 probas 形状为 [1, 50257]\n",
    "        #   每个元素表示对应 token 的概率，所有概率之和为 1\n",
    "        #   概率最高的 token 就是模型认为最可能的下一个 token\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # 获取概率最高的词汇表条目的索引：使用贪婪解码选择下一个 token\n",
    "        # torch.argmax(probas, dim=-1, keepdim=True): 找到概率最大的 token 索引\n",
    "        #   probas: 形状为 (batch, vocab_size) 的概率分布\n",
    "        #   dim=-1: 在最后一个维度（词汇表维度）上查找最大值\n",
    "        #   keepdim=True: 保持维度，使输出形状为 (batch, 1) 而不是 (batch,)\n",
    "        #   输出: idx_next，形状为 (batch, 1) 的整数张量\n",
    "        #     例如：如果 batch=1，则 idx_next 形状为 [1, 1]\n",
    "        #     值可能是 [[1234]]，表示 token ID 1234 是概率最高的下一个 token\n",
    "        #   这是贪婪解码的核心：总是选择概率最高的 token，不考虑其他可能性\n",
    "        #   优点：速度快、确定性好（相同输入总是生成相同输出）\n",
    "        #   缺点：可能生成重复或不够多样化的文本\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # 将采样的索引追加到运行序列中：将新生成的 token 添加到现有序列\n",
    "        # torch.cat((idx, idx_next), dim=1): 沿着序列维度（第1维）拼接张量\n",
    "        #   idx: 原始序列，形状为 (batch, n_tokens)，例如 [1, 10]\n",
    "        #   idx_next: 新生成的 token，形状为 (batch, 1)，例如 [1, 1]\n",
    "        #   dim=1: 指定在第1维（序列维度）上拼接\n",
    "        #   输出: idx，形状为 (batch, n_tokens+1)，例如 [1, 11]\n",
    "        #   例如：\n",
    "        #     - idx = [[1234, 5678, 9012]]，形状 [1, 3]\n",
    "        #     - idx_next = [[3456]]，形状 [1, 1]\n",
    "        #     - 拼接后: idx = [[1234, 5678, 9012, 3456]]，形状 [1, 4]\n",
    "        #   这个更新后的 idx 会在下一次迭代中作为新的上下文使用\n",
    "        #   序列长度每次增加 1，直到生成 max_new_tokens 个新 token\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    # 返回生成的完整序列：包含原始输入和新生成的所有 token\n",
    "    # idx: 形状为 (batch, n_tokens + max_new_tokens) 的整数张量\n",
    "    #   例如：如果原始输入有 10 个 token，生成了 50 个新 token\n",
    "    #   则返回的 idx 形状为 [batch, 60]，包含 60 个 token 的完整序列\n",
    "    #   这个序列可以用于：\n",
    "    #     - 解码为文本：使用 tokenizer 将 token ID 转换回文本\n",
    "    #     - 进一步处理：作为其他任务的输入\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6515f2c1-3cc7-421c-8d58-cc2f563b7030",
   "metadata": {},
   "source": [
    "- 上面的`generate_text_simple`实现了一个迭代过程，它一次创建一个token\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/18.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682eac4-f9bd-438b-9dec-6b1cc7bc05ce",
   "metadata": {},
   "source": [
    "- 让我们准备一个输入示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e3e94-df0f-4c0f-a6a1-423f500ac1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a72a9b60-de66-44cf-b2f9-1e638934ada4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() # disable dropout\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d131c00-1787-44ba-bec3-7c145497b2c3",
   "metadata": {},
   "source": [
    "- 移除批次维度并转换回文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d99f6-5710-4446-8d52-117fb34ea9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a894003-51f6-4ccc-996f-3b9c7d5a1d70",
   "metadata": {},
   "source": [
    "- 请注意，模型未经过训练；因此上面的输出文本是随机的\n",
    "- 我们将在下一章训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35278b6-9e5c-480f-83e5-011a1173648f",
   "metadata": {},
   "source": [
    "## 总结和要点\n",
    "\n",
    "- 请参见[./gpt.py](./gpt.py)脚本，这是一个包含我们在此Jupyter notebook中实现的GPT模型的自包含脚本\n",
    "- 您可以在[./exercise-solutions.ipynb](./exercise-solutions.ipynb)中找到练习解决方案"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
